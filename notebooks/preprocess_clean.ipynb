{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Pipeline de Preprocesamiento de Datos de Rasgos Cuantitativos de Arroz\n",
    "\n",
    "**Objetivo:** Realizar un an√°lisis exploratorio de datos (EDA) robusto y preparar los datos de rasgos fenot√≠picos de arroz para an√°lisis posteriores.\n",
    "\n",
    "**Autor:** [Tu nombre]  \n",
    "**Fecha:** Octubre 2025  \n",
    "**Dataset:** Rasgos cuantitativos de arroz (2,266 accesiones √ó 12 rasgos)\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "1. [Configuraci√≥n e Importaci√≥n de Librer√≠as](#1-configuraci√≥n)\n",
    "2. [Carga de Datos](#2-carga-de-datos)\n",
    "3. [Exploraci√≥n Inicial](#3-exploraci√≥n-inicial)\n",
    "4. [Limpieza de Datos](#4-limpieza)\n",
    "5. [Tratamiento de Valores Faltantes](#5-valores-faltantes)\n",
    "6. [Detecci√≥n de Outliers](#6-outliers)\n",
    "7. [Estandarizaci√≥n](#7-estandarizaci√≥n)\n",
    "8. [Enriquecimiento con Metadatos](#8-metadatos)\n",
    "9. [Alineaci√≥n con Datos Gen√≥micos](#9-alineaci√≥n)\n",
    "10. [Exportaci√≥n de Resultados](#10-exportaci√≥n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n e Importaci√≥n de Librer√≠as\n",
    "\n",
    "Configuraci√≥n del entorno de trabajo e importaci√≥n de las librer√≠as necesarias para el an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTACI√ìN DE LIBRER√çAS\n",
    "# ============================================================================\n",
    "\n",
    "# Manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geolocalizaci√≥n\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import pycountry\n",
    "\n",
    "# Utilidades\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN DE VISUALIZACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "# Configuraci√≥n de pandas\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# Configuraci√≥n de seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (12, 6),\n",
    "    \"figure.dpi\": 100,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"figure.titlesize\": 16\n",
    "})\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")\n",
    "print(f\"‚úì Pandas versi√≥n: {pd.__version__}\")\n",
    "print(f\"‚úì NumPy versi√≥n: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Carga del dataset de rasgos cuantitativos y configuraci√≥n de rutas de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEFINICI√ìN DE RUTAS\n",
    "# ============================================================================\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "TRAITS_PATH = DATA_DIR / \"trait_data\" / \"quantitative_traits.csv\"\n",
    "REPORTS_DIR = ROOT / \"reports\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CARGA DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "df = pd.read_csv(TRAITS_PATH, index_col=0)\n",
    "\n",
    "# Limpieza inicial de nombres de columnas y tipos de datos\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "df.index = df.index.astype(str).str.strip()\n",
    "\n",
    "print(f\"‚úì Dataset cargado exitosamente\")\n",
    "print(f\"  Dimensiones: {df.shape[0]:,} muestras √ó {df.shape[1]} rasgos\")\n",
    "print(f\"  Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trait_description",
   "metadata": {},
   "source": [
    "## 3. Exploraci√≥n Inicial\n",
    "\n",
    "### 3.1 Descripci√≥n de Rasgos\n",
    "\n",
    "El dataset contiene 12 rasgos cuantitativos de arroz:\n",
    "\n",
    "| C√≥digo | Descripci√≥n en Ingl√©s | Descripci√≥n en Espa√±ol | Unidad |\n",
    "|--------|----------------------|------------------------|--------|\n",
    "| **CUDI_REPRO** | Culm diameter | Di√°metro del tallo | mm |\n",
    "| **CULT_REPRO** | Culm length | Longitud del tallo | cm |\n",
    "| **CUNO_REPRO** | Culm number | N√∫mero de tallos | unidades |\n",
    "| **GRLT** | Grain length | Longitud del grano | mm |\n",
    "| **GRWD** | Grain width | Ancho del grano | mm |\n",
    "| **GRWT100** | 100-grain weight | Peso de 100 granos | g |\n",
    "| **HDG_80HEAD** | Heading date (80% flowering) | Fecha de floraci√≥n (80%) | d√≠as |\n",
    "| **LIGLT** | Ligule length | Longitud de la l√≠gula | mm |\n",
    "| **LLT** | Leaf length | Longitud de la hoja | cm |\n",
    "| **LWD** | Leaf width | Ancho de la hoja | cm |\n",
    "| **PLT_POST** | Panicle length | Longitud de la pan√≠cula | cm |\n",
    "| **SDHT** | Seedling height | Altura de pl√°ntula | cm |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESUMEN ESTAD√çSTICO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\\n\")\n",
    "\n",
    "print(\"Columnas del dataset:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRIMERAS 5 MUESTRAS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing_analysis",
   "metadata": {},
   "source": [
    "### 3.2 An√°lisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing_values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISIS DE VALORES FALTANTES\n",
    "# ============================================================================\n",
    "\n",
    "# Calcular porcentaje de valores faltantes por rasgo\n",
    "missing_traits = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES POR RASGO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Rasgo':<15} {'Faltantes':<12} {'Porcentaje'}\")\n",
    "print(\"-\" * 45)\n",
    "for trait, pct in missing_traits.items():\n",
    "    print(f\"{trait:<15} {df[trait].isna().sum():<12} {pct:>6.2f}%\")\n",
    "\n",
    "# Calcular porcentaje de valores faltantes por muestra\n",
    "missing_samples = df.isna().sum(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DISTRIBUCI√ìN DE VALORES FALTANTES POR MUESTRA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMuestras sin valores faltantes: {(missing_samples == 0).sum():,} ({(missing_samples == 0).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Muestras con 1-3 faltantes: {((missing_samples > 0) & (missing_samples <= 3)).sum():,}\")\n",
    "print(f\"Muestras con 4-6 faltantes: {((missing_samples > 3) & (missing_samples <= 6)).sum():,}\")\n",
    "print(f\"Muestras con >6 faltantes: {(missing_samples > 6).sum():,}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras por rasgo\n",
    "missing_traits.plot(kind='barh', ax=axes[0], color='coral')\n",
    "axes[0].set_xlabel('Porcentaje de valores faltantes (%)')\n",
    "axes[0].set_title('Valores Faltantes por Rasgo')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Histograma por muestra\n",
    "axes[1].hist(missing_samples, bins=13, color='skyblue', edgecolor='black')\n",
    "axes[1].set_xlabel('N√∫mero de rasgos faltantes')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title('Distribuci√≥n de Faltantes por Muestra')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Gr√°fico guardado en: {REPORTS_DIR / 'missing_values_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Limpieza de Datos\n",
    "\n",
    "### 4.1 Eliminaci√≥n de Duplicados y Columnas Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIMPIEZA: DUPLICADOS Y COLUMNAS CONSTANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIMPIEZA DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar duplicados\n",
    "duplicates = df.duplicated()\n",
    "print(f\"\\nüìå Filas duplicadas encontradas: {duplicates.sum()}\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"   Eliminando duplicados...\")\n",
    "    df = df[~duplicates]\n",
    "\n",
    "# Verificar columnas constantes\n",
    "constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]\n",
    "print(f\"\\nüìå Columnas constantes encontradas: {len(constant_cols)}\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"   Columnas: {constant_cols}\")\n",
    "    print(\"   Eliminando columnas constantes...\")\n",
    "    df = df.drop(columns=constant_cols)\n",
    "\n",
    "print(f\"\\n‚úì Dimensiones despu√©s de limpieza: {df.shape[0]:,} √ó {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Tratamiento de Valores Faltantes\n",
    "\n",
    "### 5.1 Filtrado por Umbrales\n",
    "\n",
    "Aplicamos umbrales de calidad para eliminar rasgos y muestras con exceso de valores faltantes:\n",
    "- **Rasgos:** Eliminar si >20% de valores faltantes\n",
    "- **Muestras:** Eliminar si >50% de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold_filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILTRADO POR UMBRALES DE VALORES FALTANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FILTRADO POR UMBRALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Umbrales\n",
    "TRAIT_THRESHOLD = 0.20  # 20% m√°ximo de faltantes por rasgo\n",
    "SAMPLE_THRESHOLD = 0.50  # 50% m√°ximo de faltantes por muestra\n",
    "\n",
    "original_shape = df.shape\n",
    "\n",
    "# 1. Filtrar rasgos\n",
    "trait_missing_pct = df.isna().mean()\n",
    "traits_to_keep = trait_missing_pct[trait_missing_pct <= TRAIT_THRESHOLD].index\n",
    "traits_removed = df.columns.difference(traits_to_keep)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ FILTRADO DE RASGOS (umbral: {TRAIT_THRESHOLD*100:.0f}%)\")\n",
    "print(f\"   Rasgos originales: {df.shape[1]}\")\n",
    "print(f\"   Rasgos eliminados: {len(traits_removed)}\")\n",
    "if len(traits_removed) > 0:\n",
    "    for trait in traits_removed:\n",
    "        pct = trait_missing_pct[trait] * 100\n",
    "        print(f\"     - {trait}: {pct:.1f}% faltantes\")\n",
    "\n",
    "df = df[traits_to_keep]\n",
    "\n",
    "# 2. Filtrar muestras\n",
    "sample_missing_pct = df.isna().mean(axis=1)\n",
    "samples_to_keep = sample_missing_pct[sample_missing_pct <= SAMPLE_THRESHOLD].index\n",
    "samples_removed = len(df) - len(samples_to_keep)\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ FILTRADO DE MUESTRAS (umbral: {SAMPLE_THRESHOLD*100:.0f}%)\")\n",
    "print(f\"   Muestras originales: {original_shape[0]:,}\")\n",
    "print(f\"   Muestras eliminadas: {samples_removed:,}\")\n",
    "\n",
    "df = df.loc[samples_to_keep]\n",
    "\n",
    "print(f\"\\n‚úì Dimensiones despu√©s del filtrado: {df.shape[0]:,} √ó {df.shape[1]}\")\n",
    "print(f\"  Reducci√≥n: {(1 - df.shape[0]/original_shape[0])*100:.1f}% en muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imputation_section",
   "metadata": {},
   "source": [
    "### 5.2 Imputaci√≥n de Valores Faltantes\n",
    "\n",
    "Imputamos los valores faltantes restantes usando la **mediana** de cada rasgo (m√©todo robusto ante outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPUTACI√ìN POR MEDIANA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IMPUTACI√ìN DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar faltantes antes de imputaci√≥n\n",
    "missing_before = df.isna().sum().sum()\n",
    "print(f\"\\nValores faltantes antes de imputaci√≥n: {missing_before:,}\")\n",
    "\n",
    "# Imputaci√≥n por mediana\n",
    "df_imputed = df.fillna(df.median())\n",
    "\n",
    "# Verificar imputaci√≥n\n",
    "missing_after = df_imputed.isna().sum().sum()\n",
    "print(f\"Valores faltantes despu√©s de imputaci√≥n: {missing_after:,}\")\n",
    "\n",
    "if missing_after == 0:\n",
    "    print(\"\\n‚úì Todos los valores faltantes han sido imputados exitosamente\")\n",
    "    df = df_imputed\n",
    "else:\n",
    "    print(f\"\\n‚ö† A√∫n quedan {missing_after} valores faltantes (posibles columnas vac√≠as)\")\n",
    "\n",
    "# Mostrar medianas utilizadas\n",
    "print(\"\\nMedianas utilizadas para imputaci√≥n:\")\n",
    "print(df.median().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Detecci√≥n de Outliers\n",
    "\n",
    "Identificaci√≥n de valores at√≠picos usando el m√©todo del **rango intercuart√≠lico (IQR)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DETECCI√ìN DE OUTLIERS (M√âTODO IQR)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DETECCI√ìN DE OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando el m√©todo IQR.\n",
    "    \n",
    "    Par√°metros:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Serie de datos num√©ricos\n",
    "    multiplier : float\n",
    "        Multiplicador del IQR (1.5 para outliers moderados, 3 para extremos)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    pd.Series : M√°scara booleana de outliers\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "# Detectar outliers por rasgo\n",
    "outlier_summary = {}\n",
    "for col in df.columns:\n",
    "    outliers = detect_outliers_iqr(df[col])\n",
    "    outlier_summary[col] = {\n",
    "        'count': outliers.sum(),\n",
    "        'percentage': outliers.sum() / len(df) * 100\n",
    "    }\n",
    "\n",
    "# Mostrar resumen\n",
    "print(f\"\\n{'Rasgo':<15} {'Outliers':<10} {'Porcentaje'}\")\n",
    "print(\"-\" * 40)\n",
    "for trait, stats in outlier_summary.items():\n",
    "    print(f\"{trait:<15} {stats['count']:<10} {stats['percentage']:>6.2f}%\")\n",
    "\n",
    "total_outliers = sum(s['count'] for s in outlier_summary.values())\n",
    "print(f\"\\nTotal de outliers detectados: {total_outliers:,}\")\n",
    "print(f\"Porcentaje del dataset: {total_outliers/(df.shape[0]*df.shape[1])*100:.2f}%\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    axes[i].boxplot(df[col], vert=True)\n",
    "    axes[i].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[i].set_ylabel('Valor')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A√±adir conteo de outliers\n",
    "    n_outliers = outlier_summary[col]['count']\n",
    "    axes[i].text(0.5, 0.95, f'Outliers: {n_outliers}',\n",
    "                transform=axes[i].transAxes,\n",
    "                ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                fontsize=8)\n",
    "\n",
    "plt.suptitle('Distribuci√≥n de Rasgos y Detecci√≥n de Outliers', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'outliers_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Gr√°fico guardado en: {REPORTS_DIR / 'outliers_boxplots.png'}\")\n",
    "print(\"\\n‚ö† Nota: Los outliers se mantienen en el dataset (pueden ser variaci√≥n biol√≥gica real)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Estandarizaci√≥n de Datos\n",
    "\n",
    "Estandarizaci√≥n Z-score (media=0, desviaci√≥n est√°ndar=1) para an√°lisis posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standardization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ESTANDARIZACI√ìN (Z-SCORE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTANDARIZACI√ìN DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Guardar copia de datos sin estandarizar\n",
    "df_raw = df.copy()\n",
    "\n",
    "# Estandarizaci√≥n\n",
    "df_scaled = (df - df.mean()) / df.std()\n",
    "\n",
    "# Verificar estandarizaci√≥n\n",
    "print(\"\\nVerificaci√≥n de estandarizaci√≥n:\")\n",
    "print(f\"\\n{'Rasgo':<15} {'Media':<10} {'Desv. Std'}\")\n",
    "print(\"-\" * 40)\n",
    "for col in df_scaled.columns:\n",
    "    print(f\"{col:<15} {df_scaled[col].mean():>9.6f} {df_scaled[col].std():>10.6f}\")\n",
    "\n",
    "print(\"\\n‚úì Datos estandarizados correctamente (media ‚âà 0, std ‚âà 1)\")\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Antes de estandarizaci√≥n\n",
    "df_raw.boxplot(ax=axes[0], rot=45)\n",
    "axes[0].set_title('Datos Originales', fontweight='bold')\n",
    "axes[0].set_ylabel('Valor')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Despu√©s de estandarizaci√≥n\n",
    "df_scaled.boxplot(ax=axes[1], rot=45)\n",
    "axes[1].set_title('Datos Estandarizados (Z-score)', fontweight='bold')\n",
    "axes[1].set_ylabel('Z-score')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=0.8, label='Media=0')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'standardization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Gr√°fico guardado en: {REPORTS_DIR / 'standardization_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "## 8. Enriquecimiento con Metadatos\n",
    "\n",
    "### 8.1 Carga de Metadatos de Pasaporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARGA DE METADATOS DE PASAPORTE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENRIQUECIMIENTO CON METADATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cargar datos de pasaporte\n",
    "PASSPORT_PATH = DATA_DIR / \"passport_data\" / \"passport.csv\"\n",
    "\n",
    "try:\n",
    "    passport = pd.read_csv(PASSPORT_PATH, index_col=0)\n",
    "    passport.index = passport.index.astype(str).str.strip()\n",
    "    \n",
    "    print(f\"\\n‚úì Metadatos cargados: {passport.shape[0]:,} accesiones\")\n",
    "    print(f\"  Columnas disponibles: {', '.join(passport.columns[:5])}...\")\n",
    "    \n",
    "    # Fusionar con datos de rasgos\n",
    "    df_enriched = df.join(passport, how='left')\n",
    "    \n",
    "    print(f\"\\n‚úì Datos enriquecidos: {df_enriched.shape[0]:,} √ó {df_enriched.shape[1]} columnas\")\n",
    "    \n",
    "    # Resumen de informaci√≥n geogr√°fica\n",
    "    if 'country' in df_enriched.columns:\n",
    "        print(f\"\\nDistribuci√≥n geogr√°fica:\")\n",
    "        country_counts = df_enriched['country'].value_counts().head(10)\n",
    "        for country, count in country_counts.items():\n",
    "            print(f\"  {country}: {count:,} accesiones\")\n",
    "    \n",
    "    df_clean = df_enriched.copy()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ö† Archivo de metadatos no encontrado: {PASSPORT_PATH}\")\n",
    "    print(\"  Continuando sin enriquecimiento...\")\n",
    "    df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geocoding_section",
   "metadata": {},
   "source": [
    "### 8.2 Geocodificaci√≥n de Pa√≠ses\n",
    "\n",
    "**Nota:** Esta secci√≥n requiere conexi√≥n a internet y puede tardar varios minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geocoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GEOCODIFICACI√ìN (OPCIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GEOCODIFICACI√ìN DE PA√çSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ENABLE_GEOCODING = False  # Cambiar a True para ejecutar\n",
    "\n",
    "if ENABLE_GEOCODING and 'country' in df_clean.columns:\n",
    "    \n",
    "    def normalize_country(name):\n",
    "        \"\"\"Normaliza nombres de pa√≠ses usando pycountry\"\"\"\n",
    "        try:\n",
    "            return pycountry.countries.lookup(name).name\n",
    "        except LookupError:\n",
    "            return name\n",
    "    \n",
    "    def get_coordinates(country_name, geolocator):\n",
    "        \"\"\"Obtiene coordenadas del centroide de un pa√≠s\"\"\"\n",
    "        try:\n",
    "            location = geolocator.geocode(country_name, timeout=10)\n",
    "            if location is None:\n",
    "                location = geolocator.geocode(country_name.replace('_', ' '), timeout=10)\n",
    "            \n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {country_name} - {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    # Inicializar geocodificador\n",
    "    geolocator = Nominatim(user_agent=\"rice_analysis_2025\")\n",
    "    \n",
    "    # Obtener pa√≠ses √∫nicos\n",
    "    countries = df_clean['country'].dropna().unique()\n",
    "    coordinates_map = {}\n",
    "    \n",
    "    print(f\"\\nObteniendo coordenadas para {len(countries)} pa√≠ses...\\n\")\n",
    "    \n",
    "    for i, country in enumerate(countries, 1):\n",
    "        lat, lon = get_coordinates(country, geolocator)\n",
    "        if lat and lon:\n",
    "            coordinates_map[country] = [lat, lon]\n",
    "            print(f\"  {i:2d}. ‚úì {country:<30} {lat:>7.2f}¬∞, {lon:>7.2f}¬∞\")\n",
    "        else:\n",
    "            print(f\"  {i:2d}. ‚úó {country:<30} (no encontrado)\")\n",
    "        time.sleep(1)  # Respetar l√≠mites de API\n",
    "    \n",
    "    # Guardar coordenadas\n",
    "    coords_path = DATA_DIR / \"country_coordinates.json\"\n",
    "    with open(coords_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(coordinates_map, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Coordenadas guardadas en: {coords_path}\")\n",
    "    \n",
    "    # A√±adir coordenadas al DataFrame\n",
    "    df_clean['latitude'] = df_clean['country'].map(\n",
    "        lambda x: coordinates_map[x][0] if x in coordinates_map else None\n",
    "    )\n",
    "    df_clean['longitude'] = df_clean['country'].map(\n",
    "        lambda x: coordinates_map[x][1] if x in coordinates_map else None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Accesiones geocodificadas: {df_clean['latitude'].notna().sum():,} de {len(df_clean):,}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚äò Geocodificaci√≥n deshabilitada\")\n",
    "    print(\"  Para habilitar: establece ENABLE_GEOCODING = True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "## 9. Alineaci√≥n con Datos Gen√≥micos\n",
    "\n",
    "Alineaci√≥n de muestras entre datos fenot√≠picos y gen√≥micos para an√°lisis integrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ALINEACI√ìN CON DATOS GEN√ìMICOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALINEACI√ìN CON DATOS GEN√ìMICOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "GENO_PATH = DATA_DIR / \"genotype_data\" / \"genotype.csv\"\n",
    "\n",
    "try:\n",
    "    # Cargar datos gen√≥micos\n",
    "    geno = pd.read_csv(GENO_PATH, index_col=0)\n",
    "    geno.index = geno.index.astype(str).str.strip()\n",
    "    \n",
    "    print(f\"\\n‚úì Datos gen√≥micos cargados: {geno.shape[0]:,} √ó {geno.shape[1]:,} SNPs\")\n",
    "    \n",
    "    # Identificar muestras comunes\n",
    "    traits_ids = set(df_clean.index)\n",
    "    geno_ids = set(geno.index)\n",
    "    common_ids = traits_ids.intersection(geno_ids)\n",
    "    \n",
    "    print(f\"\\nAn√°lisis de superposici√≥n:\")\n",
    "    print(f\"  Muestras en rasgos: {len(traits_ids):,}\")\n",
    "    print(f\"  Muestras en genotipos: {len(geno_ids):,}\")\n",
    "    print(f\"  Muestras en com√∫n: {len(common_ids):,}\")\n",
    "    print(f\"  Solo en rasgos: {len(traits_ids - geno_ids):,}\")\n",
    "    print(f\"  Solo en genotipos: {len(geno_ids - traits_ids):,}\")\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"\\n‚ö† ERROR: No hay muestras en com√∫n\")\n",
    "        print(\"  Verificar formato de IDs de muestra\")\n",
    "    else:\n",
    "        # Crear subconjuntos alineados\n",
    "        common_ids_sorted = sorted(common_ids)\n",
    "        traits_aligned = df_clean.loc[common_ids_sorted].copy()\n",
    "        geno_aligned = geno.loc[common_ids_sorted].copy()\n",
    "        \n",
    "        print(f\"\\n‚úì Alineaci√≥n exitosa:\")\n",
    "        print(f\"  Rasgos alineados: {traits_aligned.shape}\")\n",
    "        print(f\"  Genotipos alineados: {geno_aligned.shape}\")\n",
    "        \n",
    "        # Guardar datasets alineados\n",
    "        traits_aligned.to_csv(REPORTS_DIR / \"traits_aligned.csv\")\n",
    "        geno_aligned.to_csv(REPORTS_DIR / \"genotypes_aligned.csv\")\n",
    "        \n",
    "        print(f\"\\n‚úì Datasets alineados guardados en {REPORTS_DIR}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ö† Archivo de genotipos no encontrado: {GENO_PATH}\")\n",
    "    print(\"  Omitiendo alineaci√≥n gen√≥mica...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† Error durante alineaci√≥n: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10",
   "metadata": {},
   "source": [
    "## 10. Exportaci√≥n de Resultados\n",
    "\n",
    "Guardado de todos los datasets procesados y reporte de resumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORTACI√ìN DE RESULTADOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTACI√ìN DE RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear diccionario de datasets\n",
    "datasets = {\n",
    "    'traits_raw': df_raw,\n",
    "    'traits_clean': df_clean,\n",
    "    'traits_scaled': df_scaled\n",
    "}\n",
    "\n",
    "# Guardar datasets\n",
    "print(\"\\nGuardando datasets procesados...\")\n",
    "for name, dataset in datasets.items():\n",
    "    output_path = REPORTS_DIR / f\"{name}.csv\"\n",
    "    dataset.to_csv(output_path)\n",
    "    print(f\"  ‚úì {name}.csv - {dataset.shape[0]:,} √ó {dataset.shape[1]} columnas\")\n",
    "\n",
    "# Crear reporte de resumen\n",
    "summary_report = {\n",
    "    'Informaci√≥n General': {\n",
    "        'Fecha de procesamiento': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'Dataset original': str(TRAITS_PATH),\n",
    "        'Dimensiones originales': f\"{original_shape[0]:,} √ó {original_shape[1]}\",\n",
    "        'Dimensiones finales': f\"{df_clean.shape[0]:,} √ó {df_clean.shape[1]}\"\n",
    "    },\n",
    "    'Limpieza': {\n",
    "        'Duplicados eliminados': duplicates.sum(),\n",
    "        'Columnas constantes eliminadas': len(constant_cols) if constant_cols else 0,\n",
    "        'Muestras filtradas por umbral': samples_removed,\n",
    "        'Rasgos filtrados por umbral': len(traits_removed) if len(traits_removed) > 0 else 0\n",
    "    },\n",
    "    'Valores Faltantes': {\n",
    "        'Total antes de imputaci√≥n': missing_before,\n",
    "        'Total despu√©s de imputaci√≥n': missing_after,\n",
    "        'M√©todo de imputaci√≥n': 'Mediana'\n",
    "    },\n",
    "    'Calidad de Datos': {\n",
    "        'Outliers detectados': total_outliers,\n",
    "        'Porcentaje de outliers': f\"{total_outliers/(df.shape[0]*df.shape[1])*100:.2f}%\",\n",
    "        'Datos estandarizados': 'S√≠ (Z-score)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar reporte en JSON\n",
    "report_path = REPORTS_DIR / \"preprocessing_summary.json\"\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_report, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úì Reporte de resumen guardado: {report_path}\")\n",
    "\n",
    "# Mostrar resumen en pantalla\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for section, items in summary_report.items():\n",
    "    print(f\"\\n{section}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTodos los archivos guardados en: {REPORTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones del Preprocesamiento\n",
    "\n",
    "### Resultados Clave:\n",
    "\n",
    "1. **Calidad de datos mejorada:** Se eliminaron duplicados, valores constantes y muestras/rasgos con exceso de faltantes\n",
    "2. **Imputaci√≥n robusta:** Uso de mediana para resistencia ante outliers\n",
    "3. **Estandarizaci√≥n:** Datos preparados para an√°lisis multivariado\n",
    "4. **Enriquecimiento:** Metadatos de pasaporte integrados\n",
    "5. **Alineaci√≥n gen√≥mica:** Muestras sincronizadas con datos moleculares\n",
    "\n",
    "### Archivos Generados:\n",
    "\n",
    "- `traits_raw.csv` - Datos originales limpios\n",
    "- `traits_clean.csv` - Datos imputados con metadatos\n",
    "- `traits_scaled.csv` - Datos estandarizados (Z-score)\n",
    "- `traits_aligned.csv` - Rasgos alineados con genotipos\n",
    "- `genotypes_aligned.csv` - Genotipos alineados\n",
    "- `preprocessing_summary.json` - Reporte detallado\n",
    "- Gr√°ficos: `missing_values_analysis.png`, `outliers_boxplots.png`, `standardization_comparison.png`\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "\n",
    "1. An√°lisis de componentes principales (PCA)\n",
    "2. An√°lisis de correlaciones entre rasgos\n",
    "3. Estudios de asociaci√≥n gen√≥mica (GWAS)\n",
    "4. Modelado predictivo de rasgos\n",
    "\n",
    "---\n",
    "\n",
    "**Fecha:** Octubre 2025  \n",
    "**Notebook preparado para presentaci√≥n profesional** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
